{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7946ff92-9110-46ad-9ffe-4ae7cbf9b76b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Models\n",
    "\n",
    "List of models that could be implemented as part of the numerical experiments for the paper. Could be further expanded in the future when jump processes are considered.\n",
    "\n",
    "Selection of appropriate forward and backward proposals can be reduced to the selection of an appropriate Linear SDE whose dynamics closely match that of the signal process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d2f21-382d-4b15-a051-c70393b4edbd",
   "metadata": {},
   "source": [
    "# Ornstein Uhlenbeck (1-D)\n",
    "\n",
    "# Model:\n",
    "$$dX(s) = -\\rho X(s) ds + \\phi dB(s) \\qquad X(0) = 0$$\n",
    "\n",
    "$$f_t: Y_t | E_t = e_t \\sim \\mathcal{N}(e_t, \\eta^2) \\qquad \\qquad \\theta = (\\rho, \\phi, \\eta^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc51f6-26e7-4c10-9b8e-510c3d8ce6a6",
   "metadata": {},
   "source": [
    "## Create a notebook for 1-D OU filtering:\n",
    "\n",
    "### Proposal Choices - Forward Proposals:\n",
    "\n",
    "The only choice that needs to be made for the forward proposal, is a choice of Linear SDE, with a known, Linear Gaussian transition density:\n",
    "\n",
    "- Optimal Proposal (corresponds to linearisation)\n",
    "- Scaled Brownian Motion Proposal\n",
    "- Unscaled Brownian Motion Proposal\n",
    "\n",
    "### Proposal Choices - Backward Proposals:\n",
    "\n",
    "A choice of linear SDE with known linear Gaussian transition density (that satisfies the matching condition) immediately implies a full backward proposal.\n",
    "\n",
    "The constant diffusion coefficient means that the Delyon-Hu bridge is easy to implement, and doesn't involve the final stochastic integral. Instead, it only involves the expression given in Roberts and Papaspiliopoulos (2012). \n",
    "\n",
    "#### Evaluation of Impact of Bridge Choice on Proposals\n",
    "\n",
    "- Optimal End Point Proposal, True Diffusion Bridge of OU Process (OU-SDE for end point and bridge)\n",
    "- Optimal End Point Proposal, Scaled Brownian Bridge\n",
    "- Optimal End Point Proposal, Unscaled Brownian Bridge  \n",
    "- Optimal End Point Proposal, Delyon-Hu Bridge\n",
    "\n",
    "#### Evaluation of Impact of End Point choice on Proposals\n",
    "\n",
    "- Optimal End Point Proposal, True Diffusion Bridge of OU Process (OU-SDE for end point and bridge)\n",
    "- Scaled Brownian Proposal, True Diffusion Bridge of OU Process (OU-SDE for end point and bridge)\n",
    "- Unscaled Proposal, True Diffusion Bridge of OU Process (OU-SDE for end point and bridge)\n",
    "- Driftless End Point Proposal, True Diffusion Bridge of OU Process (OU-SDE for end point and bridge)\n",
    "\n",
    "#### Other interesting Proposals\n",
    "\n",
    "- Scaled Brownian End Point Proposal, Scaled Brownian Diffusion Bridge (Scaled Brownian SDE for end point and bridge)\n",
    "- Scaled Brownian End Point Proposal, Delyon-Hu Bridge\n",
    "- Unscaled Brownian End Point Proposal, Unscaled Brownian Diffusion Bridge\n",
    "- Unscaled Brownian End Point Proposal, Delyon-Hu Bridge "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf080c93-4c83-4d2c-a084-7287966f63d3",
   "metadata": {},
   "source": [
    "## Plots: - Look through the results and decide how best to communicate the ideas\n",
    "\n",
    "- ESS for a single run\n",
    "- Root MSE of estimator of the first moment over time across different particle filters\n",
    "- Boxplots of log likelihood estimate time $T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705f592d-697d-41f6-a275-01f1d241cd11",
   "metadata": {},
   "source": [
    "# To do\n",
    "## Filtering\n",
    "- Get the filtering working on the TV CDSSM for the VanDerMeulen - Schauer bridges.\n",
    "\n",
    "## Smoothing\n",
    "- Run some experiments on the smoothing fk models that you have working already. Setup the framework to test this on different SDEs.\n",
    "\n",
    "## Particle Gibbs\n",
    "- Start looking at running a parameter inference algorithm in one dimension.\n",
    "- You can run PGBS, PG and PMMH all for the same model.\n",
    "\n",
    "## Running in higher dimensions\n",
    "- Add the higher dimensional processes\n",
    "- Start building and implementing the models in higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1248b63-7fa1-4b3b-b085-93b78918d59f",
   "metadata": {},
   "source": [
    "## Smoothing Algorithms that require a density:\n",
    "\n",
    "### Offline Smoothing: \n",
    "\n",
    "- FFBS\n",
    "- FFBS-Reject\n",
    "- FFBS-MCMC (recommended)\n",
    "- FFBS-QMC\n",
    "- Two-Filter Smoothing\n",
    "\n",
    "### Online Smoothing:\n",
    "- PaRIS $\\mathcal{O}(NT)$\n",
    "- Online Smoothing $\\mathcal{O}(N^2T)$\n",
    "\n",
    "### MCMC\n",
    "\n",
    "- Particle Gibbs with Backward/Ancestral Sampling (can you implement ancestral too?)\n",
    "- The higher dimensional methods covered in Finke et al and Finke & Corenflos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb389177-4b69-4b0a-9a49-1e2478b41367",
   "metadata": {},
   "source": [
    "### Calclulation of Path Integrals with Vectorisation in 1D:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d95fde-a86f-4f98-a544-849d00935385",
   "metadata": {},
   "source": [
    "For Girsanov path integrals (forward proposals), the inputs are:\n",
    "\n",
    "times: (num+1, )\n",
    "X: (N, num+1)\n",
    "\n",
    "`b_1` is the drift of the model sde\n",
    "`b_2` is the drift of the proposal sde\n",
    "`Cov` is the common covariance coefficient of the model and proposal sde. Taken in code from the model SDE.\n",
    "\n",
    "The model sde object will always have float values for the parameters. \n",
    "The proposal sde will (in general) have dimension (N, ) values for its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5adc264-3386-40c1-bbb2-6abbd3e0d38b",
   "metadata": {},
   "source": [
    "For Delyon-Hu bridge path integrals (backward proposals), the inputs are:\n",
    "\n",
    "times: (num+1, )\n",
    "X: (N, num+1)\n",
    "\n",
    "The auxiliary bridge has attribute x_end: of dimension (N, )\n",
    "The Delyon-Hu bridge is a particular choice of bridge construction, so we don't introduce parameters of dimension (N, ).\n",
    "The issue of calculations involving these 3 quantities is solved in the code by applying matrix transpose in various places\n",
    "to ensure that the broadcasting occurs correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02100a46-a1b2-402d-91e1-00de91a4adcd",
   "metadata": {},
   "source": [
    "For van-der-Meulen Schauer bridge path integrals (backward proposals): the inputs are:\n",
    "\n",
    "- times: (num+1, )\n",
    "- X: (N, num+1)\n",
    "- \n",
    "`b`, `b_tilde`, `Cov`, `Cov_tilde`, `r`, `H`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446af52c-761b-485c-a651-0b7da723e42f",
   "metadata": {},
   "source": [
    "### The Bridge of the Optimal Forward Proposal of the OU process:\n",
    "\n",
    "$$M_t^{\\rightarrow}[e_{t-1}, dv_t] \\qquad dX_t(s) = b_t^{\\rightarrow, opt}(s, X_t(s); y_t)dx + \\sigma_t(s, X_t(s))dB(s) \\qquad X(0) = e_{t-1}$$\n",
    "\n",
    "In this case, we have the following expression for $b_t^{\\rightarrow}$:\n",
    "\n",
    "$$b_t^{\\rightarrow, opt}(s, x; y_t) = -\\rho x + \\phi^2 a(s, \\Delta_t)\\frac{y_t - a(s, \\Delta_t)x - b(s, \\Delta_t)}{L^2v(s, \\Delta_t) + \\sigma_Y^2}$$\n",
    "\n",
    "\n",
    "- $a(s, t) = e^{-\\rho(t-s)}$\n",
    "- $b(s, t) = \\mu (1-e^{-\\rho(t-s)})$\n",
    "- $c(s, t) = \\frac{\\phi^2}{2\\rho}(1 - e^{-2\\rho (t-s)})$\n",
    "\n",
    "This forward proposal, is itself a Linear SDE, with the following expressions for $A(t)$, $B(t)$, $C(t)$:\n",
    "\n",
    "- $A(s) = \\phi^2 a(s, \\Delta_t)\\frac{y_t - b(s, \\Delta_t)}{L^2v(s, \\Delta_t) + \\sigma_Y^2}$\n",
    "- $B(s) = -\\rho - \\phi^2 \\frac{a(s, \\Delta_t)^2}{L^2v(s, \\Delta_t) + \\sigma_Y^2}$\n",
    "- $C(s) = \\phi$\n",
    "\n",
    "We want to choose our auxiliary bridge process to be the diffusion bridge of the proposal SDE. Setting $K = L^2 \\frac{\\phi^2}{2\\rho}$, this gives the following complete expressions:\n",
    "\n",
    "- $A(s) = \\phi^2 e^{-\\rho (\\Delta_t-s)}\\frac{y_t - \\mu (1-e^{-\\rho(\\Delta_t-s)})}{K(1 - e^{-2\\rho (\\Delta_t -s)}) + \\sigma_Y^2}$\n",
    "- $B(s) = -\\rho - \\phi^2 \\frac{e^{-2\\rho (\\Delta_t-s)}}{K(1 - e^{-2\\rho (\\Delta_t -s)}) + \\sigma_Y^2}$\n",
    "- $C(s) = \\phi$\n",
    "\n",
    "Recall that the general Linear SDE, given by:\n",
    "\n",
    "$$dX(s) = A(s) + B(s)X(s)ds + C(s)dB(s) \\qquad X(0) = x(0)$$\n",
    "\n",
    "can by solved analytically by using the integrating factor:\n",
    "\n",
    "$$dY(s) = -B(s)Y(s)ds \\qquad Y(0) = 1.$$\n",
    "\n",
    "by applying Ito's Formula, we define $Z(s) = X(s)Y(s)$, and thus obtain that:\n",
    "\n",
    "$$dZ(s) = A(s)Y(s)ds + C(s)Y(s)dB(s) \\qquad Z(0) = x(0)$$\n",
    "\n",
    "\n",
    "Hence, we have: defining $U(s) = Y(s)^{-1}$, for $s_2 > s_1$:\n",
    "\n",
    "$$X(s_2) = X(s_1)\\frac{Y(s_1)}{Y(s_2)} + U(s_2)\\int_{s_1}^{s_2} A(u)Y(u) du + U(s_2)\\int_{s_1}^{s_2} C(u)Y(u) dB(u)$$\n",
    "\n",
    "\n",
    "The general solution to this ODE, is of the form:\n",
    "\n",
    "$$Y(s) = \\exp[-\\int_0^s B(u) du]$$\n",
    "\n",
    "$$I(s) = -\\int_0^s B(u) du$$\n",
    "\n",
    "For the particular choice of $B(u)$, the integral can be solved analytically, and is given by (integral done with Chat GPT):\n",
    "\n",
    "$$I(s) = \\rho s + \\frac{\\phi^2}{2\\rho}\\log(\\frac{K + \\sigma_Y^2 - Ke^{-2\\rho \\Delta_t}}{K + \\sigma_Y^2 - Ke^{-2\\rho (\\Delta_t-s)}})$$\n",
    "\n",
    "$$Y(s) = e^{\\rho s}(\\frac{K + \\sigma_Y^2 - Ke^{-2\\rho \\Delta_t}}{K + \\sigma_Y^2 - Ke^{-2\\rho (\\Delta_t-s)}})^{\\frac{\\phi^2}{2\\rho}}$$\n",
    "\n",
    "To evaluate the transition density of this Linear SDE exactly, we would be required to analytically evaluate the integrals, for $s_2 > s_1$\n",
    "\n",
    "$$I_1(s_1, s_2) = \\int_{s_1}^{s_2} A(u)Y(u) du \\qquad I_2(s_1, s_2) = \\int_{s_1}^{s_2} C^2(u)Y^2(u)du$$\n",
    "\n",
    "Both of these integrals cannot be evaluated analytically, so can cannot analytically find the transition density of the Optimal Forward Proposal of the OU SDE. Thus, we cannot find analytically an expression for the true diffusion bridge of the Optimal Forward Proposal, due to the appearance of the transition density in the drift function. A possible solution to this problem, would be to evaluate the integrals by using numerical methods. This is a possible extension to the code that could be considered in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0e713-c9e3-4e6d-b94f-ed7f03cd04f3",
   "metadata": {},
   "source": [
    "Possible code extension:\n",
    "\n",
    "Choose a certain level of imputation $\\Delta s = \\frac{\\Delta_t}{K}$, where $K$ is the number of imputed points, and then, when the Linear SDE is initialised, calculate:\n",
    "\n",
    "- $B(s)$ at each point on the imputed grid. Use this to evaluate numerically $Y(s)$ at every point on the imputed grid.\n",
    "- $A(s)Y(s)$ at each point on the imputed grid. Use these points to evaluate numerically $I_1(0, s)$ at every point on the imputed grid.\n",
    "- $C(s)^2 Y(s)^2$ at each point on the imputed grid. Use these points to evaluate numerically $I_2(0, s)$ at every point on the imputed grid.\n",
    "\n",
    "These output can then be used to construct the usual functions $a(s,t)$ $b(s, t)$ and $v(s, t)$ for the Linear SDE, as long as the input points $s$ and $t$ are on the imputed grid.\n",
    "\n",
    "Using this approach, we can find numerically the transition density of a general 1D linear SDE, which can be used to construct an approximation of the diffusion bridge, or to construct forward proposals.\n",
    "\n",
    "Note that $K$ must be chosen such that `num` divides $K$. The simplest approach would be to take thm to be equal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020e607-4fde-4cd6-a380-ff6fbb47a62b",
   "metadata": {},
   "source": [
    "The same principle can be applied to construct a numerical approximation of the transition density in the case where $B(s) = 0$. In this case, we only need to evaluate the integrals $I_3(0, s)$ and $I_4(0, s)$, where these integrals are given by:\n",
    "\n",
    "$$I_3(s_1, s_2) = \\int_{s_1}^{s_2} A(u) du \\qquad I_4(s_1, s_2) = \\int_{s_1}^{s_2} C^2(u)du$$\n",
    "\n",
    "However, we should leave this idea for now, since this level of generality is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4713a04-3f84-4b82-a8ea-ef3ae44ef44b",
   "metadata": {},
   "source": [
    "The function that we are linearising, is given by:\n",
    "\n",
    "$$b_t^{\\rightarrow, opt}(s, x; y_t) = -\\rho x + \\phi^2 a(s, \\Delta_t)\\frac{y_t - a(s, \\Delta_t)x - b(s, \\Delta_t)}{L^2v(s, \\Delta_t) + \\sigma_Y^2}$$\n",
    "\n",
    "Taking the partial derivative w.r.t x, we obtain:\n",
    "\n",
    "$$\\tilde{b}_t^{\\rightarrow, opt}(s, x; y_t) = -\\rho -  \\phi^2 \\frac{a^2(s, \\Delta_t)}{L^2 v(s, \\Delta_t) + \\sigma_Y^2}$$\n",
    "\n",
    "To locally linearise the process in the code, we take:\n",
    "\n",
    "- $\\tilde{A} = b_t(\\Delta_t, x_{end}; y_t) - \\tilde{b}_t^{\\rightarrow, opt}(\\Delta_t, x_{end}; y_t)*x_{end}$\n",
    "- $\\tilde{B} = \\tilde{b}_t^{\\rightarrow, opt}(\\Delta_t, x_{end}; y_t)$\n",
    "\n",
    "So, we therefore have:\n",
    "\n",
    "- $\\tilde{A} = -\\rho x_{end} + \\phi^2 \\frac{y_t - x_{end}}{\\sigma_Y^2} + \\rho x_{end} + \\phi^2 \\frac{x_{end}}{\\sigma_Y^2} = \\frac{y_t}{\\sigma_Y^2}$\n",
    "- $\\tilde{B} = -\\rho - \\phi^2 * \\frac{1}{\\sigma_Y^2}$\n",
    "- $\\tilde{C} = \\phi$\n",
    "\n",
    "\n",
    "So we have that $\\tilde{\\rho} \\tilde{\\mu} = \\frac{y_t}{\\sigma_Y^2}$ and we have that $\\tilde{\\rho} = \\rho + \\phi^2 \\frac{1}{\\sigma_Y^2}$\n",
    "\n",
    "Hence, the local linear OU approximation of the optimal forward proposal for the OU process, is given by:\n",
    "\n",
    "- $\\tilde{\\rho} = \\rho + \\phi^2 \\frac{1}{\\sigma_Y^2}$\n",
    "- $\\tilde{\\mu} = \\frac{y_t}{\\sigma_Y^2}(\\rho + \\frac{\\phi^2}{\\sigma_Y^2})^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3a785-8fb6-4fd4-b0a1-f21d4824abb2",
   "metadata": {},
   "source": [
    "Say that we have a multivariate linear, Gaussian transition density:\n",
    "$$X_t | X_s = x_s \\sim \\mathcal{N}(Ax + b, \\Sigma)$$\n",
    "\n",
    "We can find the gradient of the transition density to be:\n",
    "\n",
    "$$ \\nabla_{x_s} \\log(p_{s, t}(x_t|x_s)) = [A^T\\Sigma^{-1}(x_t - b) -A^T\\Sigma^{-1}Ax_s]$$\n",
    "\n",
    "Where $K = A^T\\Sigma^{-1}A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32c2b5d-60d5-4676-ba0f-b3dcf4b2c80f",
   "metadata": {},
   "source": [
    "# Notes on the filtering results from the TV CDSSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a40958-cbd8-4923-bd05-a8a1fa7c3915",
   "metadata": {},
   "source": [
    "## Bootstrap DA\n",
    "\n",
    "- All reparameterised model results appear to match the original parameterisation.\n",
    "- the results from all DA Bootstrap filters match results from the filter without the use of data augmentation.\n",
    "\n",
    "## Forward Proposals\n",
    "\n",
    "- Use of any auxiliary bridge other than Delyon & Hu causes the results from reparameterised fk models to not match the original parameterisation.\n",
    "- Some (negative) bias is introduced into estimates of the log likelihood.\n",
    "- The filters track the filtering distribution well, except at the last timestep.\n",
    "- For the fk models that are not working, the ESS drops significantly in the first timestep, sometimes all the way to 0.\n",
    "\n",
    "## Backward Proposals\n",
    "\n",
    "- Use of any auxiliary bridge other than Delyon & Hu causes the results from reparameterised fk models to not match the original parameterisation.\n",
    "- Some (postive) bias is introduced into estimates of the log likelihood.\n",
    "- The filters track the filtering distribution well, except at the last timestep.\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "- The DA bootstrap models should be very close to the standard bootstrap. The difference may be caused by bias introduced by the imputation. Increasing the number of imputed points may help with this.\n",
    "- For the backward proposals, given the observation noise is low, they should be able to propose points that are very close to $y_T \\approx 0.44$. Instead, they all propose around $0.32$, which is close to the filtering mean at time $T-1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e789df6-04e2-469a-a5f1-4d6802afaeab",
   "metadata": {},
   "source": [
    "# Score function of the OU process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f33ca0-03d3-4466-912f-f10e5b3e099a",
   "metadata": {},
   "source": [
    "$\\theta = (\\rho, \\mu, \\phi, \\eta)$\n",
    "\n",
    "$E_t | E_{t-1} = e_{t-1} \\sim \\mathcal{N}(a(\\theta)e_{t-1} + b(\\theta), v(\\theta))$\n",
    "\n",
    "$\\nabla_{\\theta}\\log(p_\\theta(x_t|x_{t-1})) = -\\frac{1}{2}v'(\\theta) - \\frac{1}{2}\\frac{v(\\theta)k'(\\theta) - k(\\theta)v'(\\theta)}{v(\\theta)^2}$\n",
    "\n",
    "$k(\\theta) = (x_t - a(\\theta)x_{t-1} - b(\\theta))^2$\n",
    "\n",
    "$k'(\\theta) = -2(a'(\\theta)x_{t-1} + b'(\\theta))(x_t - a(\\theta)x_{t-1} - b(\\theta))$\n",
    "\n",
    "Given these expressions, all that is left to do is take the derivatives of $a$, $b$ and $v$ with respect to $(\\rho, \\mu, \\phi)$\n",
    "\n",
    "$v(\\theta) = \\frac{\\phi^2}{2\\rho}(1-e^{-2\\rho(t-s)}) \\quad \\frac{dv}{d\\rho} = \\frac{\\phi^2}{2\\rho}(2((t-s)+\\frac{1}{2\\rho})e^{-2\\rho(t-s)}-\\frac{1}{\\rho}) \\quad \\frac{dv}{d\\phi} = \\frac{\\phi}{\\rho}(1-e^{-2\\rho(t-s)}) \\quad \\frac{dv}{d\\mu} = 0$\n",
    "\n",
    "$a(\\theta) = e^{-\\rho(t-s)} \\quad \\frac{da}{d\\rho} = -(t-s)e^{-\\rho(t-s)} \\quad \\frac{da}{d\\phi} = 0 \\quad \\frac{da}{d\\mu} = 0$\n",
    "\n",
    "$b(\\theta) = \\mu(1-e^{-\\rho(t-s)}) \\quad \\frac{db}{d\\rho} = \\mu (t-s)e^{-\\rho (t-s)}  \\quad \\frac{db}{d\\phi} = 0 \\quad \\frac{db}{d\\mu} = 1 - e^{-\\rho(t-s)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e9c57-1b58-4e56-aa97-93ec82dbcaba",
   "metadata": {},
   "source": [
    "# Particle MCMC for CDSSMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4523f-3882-4955-9811-7a8535b0fc2d",
   "metadata": {},
   "source": [
    "## The Model: The OU-CDSSM process in 1D:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee5ac06-66cf-4a0a-a8cf-a47b1d6d056d",
   "metadata": {},
   "source": [
    "Consider the Ornstein-Uhlenbeck process in one-dimension:\n",
    "\n",
    "$$dX(s) = -\\rho X(s) ds + \\phi dB(s) \\qquad X(0) = 0.$$ (1)\n",
    "\n",
    "This process is observed at $T$ discrete times $s_1 < s_2 < \\dots < s_T$, with noise. So we are working with a continuous time set $\\mathcal{S} = [0, S]$ and a discrete one $\\mathcal{T} = \\{1, \\dots T\\}$. We set $E_t = X(s_t)$\n",
    "\n",
    "We assume for simplicity that observation times are equidistant $s_t - s_{t-1} = \\Delta_t = 1$, and we define each observation density $f_t(y_t|e_t)$ to be linear, Gaussian:\n",
    "\n",
    "$$Y_t |E_t = e_t \\sim \\mathcal{N}(e_t, \\eta^2)$$ (2)\n",
    "\n",
    "Equation (1)-(2) defines a CD-SSM with parameters $\\theta = (\\rho, \\phi, \\eta^2)$. As the OU process is a linear SDE, it has a tractable transition density $p_t(e_t|e_{t-1})$, given by:\n",
    "\n",
    "$$E_t|E_{t-1} = e_{t-1} \\sim \\mathcal{N}(e^{-\\rho \\Delta s}e_{t-1}, \\frac{\\phi^2}{2\\rho}(1 - e^{-2\\rho \\Delta s}))$$\n",
    "\n",
    "Thus, the random variables $(E_{1:T}, Y_{1:T})$ form a linear, Gaussian state space model, and it is possible to derive analytically the filtering and smoothing distributions, through Kalman filtering and smoothing. One can also implement particle filters/smoothers on discrete space, without data augmentation, with the optimal proposal being analytically tractable. Finally, the data augmentation approaches outlined in the contribution can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ce5ca-8490-4811-ad08-b87d1b156dc0",
   "metadata": {},
   "source": [
    "## Model Reparameterisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98980d8a-f9db-47aa-af97-8afea58be798",
   "metadata": {},
   "source": [
    "We introduce a reparaemterisation of the CDSSM, so that the distribution of $(E_{1:T}, Y_{1:T})$ can be defined similarly through the following updates: \n",
    "\n",
    "$$E_t = aE_{t-1} + \\epsilon_t \\quad \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_X^2) \\qquad (3)$$\n",
    "$$Y_t = E_{t} + \\eta_t \\quad \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma_Y^2) \\qquad (4)$$\n",
    "\n",
    "For $t \\in \\mathcal{T}$, where $X(0) = E_0 = 0$ is known. Under this model parameterisation, we define the parameter vector $\\theta^\\star = (a, \\sigma_X^2, \\sigma_Y^2)$. There is a bijective mapping between $\\theta$ and $\\theta^*$, so this can be considered as a reparameterisation of the CDSSM.\n",
    "\n",
    "- $a = e^{-\\rho \\Delta s}$\n",
    "- $\\sigma_X^2 = \\frac{\\phi^2}{2\\rho}(1 - e^{-2\\rho \\Delta s})$\n",
    "- $\\sigma_Y^2 = \\eta^2$\n",
    "\n",
    "Given data $Y_{1:T}$, by defining a prior on $\\theta^*$ that we call $\\nu(\\theta^*)$ can setup a Bayesian inference problem to infer either $\\theta^*$ or jointly the latent states $(E_{1:t}, \\theta^*)$. We can use various MCMC algorithms that use Sequential Monte Carlo methods to approach this problem, known as Particle MCMC methods. As the original model is a CDSSM, we can also use the Data Augmentation methods that have been developed.\n",
    "\n",
    "This reparameterisation is motivated by the application of these algorithms. Specifically, idealised algorithms exist for Linear, Gaussian state space models (e.g IMMH) that serve as idealised versions of Particle MCMC algorithms. It is more natural to consider such methods in the context of a natural parameterisation for a Linear, Gaussian State Space model. \n",
    "\n",
    "Further, under this parameterisation it is possible to select a conjugate prior for this model, assuming that both ($E_{1:T}, Y_{1:T}$) are observed. This selection enables one to do an update of $\\theta | E_{1:T}, Y_{1:T}$ within a Gibbs sampler by sampling from the true conditional distribution, as opposed to using a Metropolis step within the sampler. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3675c5-ced1-41e3-be35-c75309fd57ae",
   "metadata": {},
   "source": [
    "## Selection of the Prior distribution - Go through this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b52e6-08e5-48d0-a177-f2a817400ffe",
   "metadata": {},
   "source": [
    "We define the prior distribution $\\nu(\\theta^*)$ as follows:\n",
    "\n",
    "- $(\\rho, \\sigma_X^2) \\sim NIG(\\mu, \\lambda, \\alpha_X, \\beta_X)$\n",
    "- $(\\sigma_Y^2) \\sim IG(\\alpha_Y, \\beta_Y)$ # Note that it may be better to fix this parameter in practice (Alex advice)\n",
    "\n",
    "Then, if we assume that the data for our model is $E_{1:T}, Y_{1:T}$, then the above prior is conjugate for this model. It is possible to analytically derive the posterior distribution of the parameter given the data for this model, it is the following:\n",
    "\n",
    "- $(\\rho, \\sigma_X^2) | X_{1:T} \\sim NIG(\\mu', \\lambda', \\alpha', \\beta')$\n",
    "- $(\\sigma_Y^2) |X_{1:T}, Y_{1:T} \\sim IG(\\alpha'_Y, \\beta'_Y)$\n",
    "\n",
    "Where we have that:\n",
    "\n",
    "- $A = \\sum_{t=0}^{T-1} x_t^2 + \\lambda$\n",
    "- $B = \\lambda \\mu + \\sum_{t=1}^T x_t x_{t-1}$\n",
    "- $C = \\sum_{t=1}^T x_t^2 + 2\\beta + \\lambda \\mu^2$\n",
    "\n",
    "Then, expressing the posterior parameters for the distribution of $(\\rho, \\sigma_X) | X_{1:T}$ in terms of $A, B, C$:\n",
    "- $\\alpha' = \\frac{T}{2} + \\alpha$\n",
    "- $\\beta' = \\frac{1}{2}(C - \\frac{B^2}{A})$\n",
    "- $\\mu' = \\frac{B}{A}$\n",
    "- $\\lambda' = A$\n",
    "\n",
    "The posterior parameters of $\\sigma_Y^2 | X_{1:T}, Y_{1:T}$ are given by:\n",
    "\n",
    "- $\\alpha_Y' = \\alpha_Y + \\frac{T}{2}$\n",
    "- $\\beta_Y' = \\beta_Y + \\frac{1}{2} \\sum_{t=1}^T (y_t - x_t)^2$\n",
    "\n",
    "With these results, we can now do some inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd0808-3ff5-42fe-87a3-58ce12493100",
   "metadata": {},
   "source": [
    "## Choice of MCMC Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2136d09d-6a49-4e93-a6c1-7124df34bbfc",
   "metadata": {},
   "source": [
    "Take as your example the OU process. Marginalising out the paths between the end points, such a process is a LGSSM, so we can implement the following MCMC algorithms to infer the parameter $\\theta^*$ or jointly $(E_{1:T}, \\theta^*)$:\n",
    "\n",
    "- IMMH with access to the true marginal likelihood $p_{\\theta}(y_{1:T})$ - **correct**\n",
    "- PMMH with varying $N$ to test performance when there is low variance. - **correct**\n",
    "- Single Site Gibbs - Conjugate theta update - **correct**\n",
    "- Single Site Gibbs - MH theta update\n",
    "- Particle Gibbs - Conjugate theta update - **correct**\n",
    "- Particle Gibbs - MH theta update\n",
    "- Particle Gibbs with the backward step - Conjugate Theta update - **correct**\n",
    "- Particle Gibbs with the backward step - MH theta update\n",
    "\n",
    "We then also have for CDSSMs:\n",
    "\n",
    "- PMMH using Forward Proposal (no need for a reparameterised fk class as only filtering)\n",
    "- Particle Gibbs - Forward Proposal - MH theta update\n",
    "- Particle Gibbs - Backward Proposal - MH theta update\n",
    "- PGBS - Forward Proposal - MH theta update\n",
    "- PGBS - Backward Proposal - MH theta update\n",
    "\n",
    "This gives a total of 13 different MCMC algorithms to debug.\n",
    "\n",
    "- Full conditional update Gibbs (will require Kalman filter simulation)\n",
    "\n",
    "Start off with this to familiarise yourself with the `mcmc.py` library, and then take it from there!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776dbc9-9c0e-45bd-9b5d-6ac5f0de2481",
   "metadata": {},
   "source": [
    "You have created the following new classes for general use:\n",
    "\n",
    "- IMMH: Idealised Marginal MH algos, that can be applied for parameter inference on LGSSMs\n",
    "- AutoGibbs: Automated full parameter updates using MWG steps - uses MetropoliswithinGibbs class.\n",
    "- AutoParticleGibbs: PG with automated full parameter udpates using MWG steps - uses MetropoliswithinGibbs class.\n",
    "- CDSSM_ParticleGibbs: PG using reparameterised CDSSM FK models, with automated full parameter updates, using MWG steps - uses CDSSM_MetropoliswithinGibbs class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b108890e-58b8-4621-8d9f-c6f02db5b575",
   "metadata": {},
   "source": [
    "## First Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969d3e02-73f9-4028-a372-958a0c889463",
   "metadata": {},
   "source": [
    "I ran and debugged the algorithms for $T=100$ timesteps, using $1000$ MCMC steps. This resulted in the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5112b60-a034-4011-9643-e88e853033f6",
   "metadata": {},
   "source": [
    "- **IMMH**: Run time: 4.8s, MSJD: 4.5\n",
    "- **PMMH**: Run time: 10.7s, MSJD: 7.0\n",
    "- **SSGibbs**: Run time: 0.4s, MSJD: 197874.9\n",
    "- **SSAutoGibbs**: Run time: 41.2s, MSJD: 0.0\n",
    "- **PGibbs**: Run time: 11.5s, MSJD: 314790.6\n",
    "- **AutoPGibbs**: Run time: 54.6s, MSJD: 0.0\n",
    "- **PGibbsBS**: Run time: 15.2s, MSJD: 284293.5\n",
    "- **AutoPGibbsBS**: Run time: 57.8s, MSJD: 0.0\n",
    "- **CDSSM_PMMH**: Run time: 41.9s, MSJD: 3.6\n",
    "- **FW_CDSSM_PG**: Run time: 84.5s, MSJD: 0.0\n",
    "- **BW_CDSSM_PG**: Run time: 59.6s, MSJD: 0.0\n",
    "- **BW_CDSSM_PGBS**: Run time: 74.5s, MSJD: 0.0\n",
    "- **FW_CDSSM_PGBS**: Run time: 112.1s, MSJD: 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d540467-cb25-45d9-bd78-b26588b10f27",
   "metadata": {},
   "source": [
    "The total run time of all the algorithms was 568.7 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48526665-8a75-4f60-b579-f1d70dfa50e8",
   "metadata": {},
   "source": [
    "#### Run times for niter=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a5d2d-d6df-482d-b1d9-28fcfef588eb",
   "metadata": {},
   "source": [
    "- MCMC algorithm IMMH finished. Run time: 422.917699709069, MSJD: 226.15912305396\n",
    "- MCMC algorithm PMMH finished. Run time: 1156.0924493749626, MSJD: 228.37935091689414\n",
    "- MCMC algorithm SSGibbs finished. Run time: 27.983020332991146, MSJD: 973.3467055182768\n",
    "- MCMC algorithm PGibbs finished. Run time: 1165.92318862502, MSJD: 1720.186074574441\n",
    "- MCMC algorithm PGibbsBS finished. Run time: 1512.8000926659442, MSJD: 1412.609163801374\n",
    "\n",
    "Total run time: 4285.7 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabbf75-1c10-4f20-8c2e-120ab5164651",
   "metadata": {},
   "source": [
    "#### Run times for niter=1000, Nsteps=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553557c-2fea-4bcc-8113-3e2fcd751478",
   "metadata": {},
   "source": [
    "- MCMC algorithm SSAutoGibbs finished. Run time: 492.66876841697376, MSJD: 2.364475633333281\n",
    "- MCMC algorithm AutoPGibbs finished. Run time: 505.24563491600566, MSJD: 1.5275133605689981\n",
    "- MCMC algorithm AutoPGibbsBS finished. Run time: 510.16826579195913, MSJD: 2.7337942300261173\n",
    "\n",
    "Total run time: 1508.1 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b769b88f-3792-43e8-983e-89cff422e8b2",
   "metadata": {},
   "source": [
    "#### Run times for niter=100, Nsteps=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33dea9-1ebb-4c8a-9882-41893d229363",
   "metadata": {},
   "source": [
    "- MCMC algorithm CDSSM_PMMH finished. Run time: 4.326336166006513, MSJD: 0.0006807386535997021\n",
    "- MCMC algorithm FW_CDSSM_PGBS finished. Run time: 303.1901090419851, MSJD: 0.12257182145364039\n",
    "- MCMC algorithm BW_CDSSM_PGBS finished. Run time: 161.39417616603896, MSJD: 0.5776958863503586\n",
    "- MCMC algorithm BW_CDSSM_PG finished. Run time: 160.71721804200206, MSJD: 0.09070488696201738\n",
    "- MCMC algorithm FW_CDSSM_PG finished. Run time: 298.9900925840484, MSJD: 0.34570081561265703\n",
    "- Total time: 928.617932000081"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a012d9-ca1f-43c4-a676-23b3736a4d72",
   "metadata": {},
   "source": [
    "#### Run times for niter=23000, Nsteps=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136522df-ed21-4e60-bd2f-f5975d969aad",
   "metadata": {},
   "source": [
    "- MCMC algorithm IMMH finished. Run time: 95.13535604206845, MSJD: 53.52646511809797\n",
    "- MCMC algorithm PMMH finished. Run time: 263.93639608402736, MSJD: 55.533539226168244\n",
    "- MCMC algorithm SSGibbs finished. Run time: 6.433977541979402, MSJD: 228.29189829263794\n",
    "- MCMC algorithm SSAutoGibbs finished. Run time: 1147.6820821660804, MSJD: 4.0953430680514575\n",
    "- MCMC algorithm PGibbs finished. Run time: 267.7983410420129, MSJD: 219.5383843231522\n",
    "- MCMC algorithm AutoPGibbs finished. Run time: 1401.8546708330978, MSJD: 7.510379642630769\n",
    "- MCMC algorithm PGibbsBS finished. Run time: 342.2828895000275, MSJD: 228.94151514681462\n",
    "- MCMC algorithm AutoPGibbsBS finished. Run time: 1477.0890752909472, MSJD: 9.439677113318712\n",
    "- MCMC algorithm CDSSM_PMMH finished. Run time: 1010.1793559170328, MSJD: 2.4864619067024045\n",
    "- MCMC algorithm FW_CDSSM_PG finished. Run time: 7995.6691050839145, MSJD: 14.40418243649925\n",
    "- MCMC algorithm BW_CDSSM_PG finished. Run time: 4603.258861584007, MSJD: 2.5234067378015217\n",
    "- MCMC algorithm BW_CDSSM_PGBS finished. Run time: 4850.449482000084, MSJD: 3.8864972710139414\n",
    "- MCMC algorithm FW_CDSSM_PGBS finished. Run time: 8519.406258875038, MSJD: 4.676462371430976\n",
    "- MCMC Algorithm runs complete. Total run time: 31981.175851960317"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc80c053-4de6-4773-8455-96757e6f2d0c",
   "metadata": {},
   "source": [
    "#### Run times for niter=80000, Nsteps=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c7273-e49f-48cd-93ac-73c17fda9ee0",
   "metadata": {},
   "source": [
    "Plot ideas:\n",
    "\n",
    "\n",
    "- Chain Pairplots\n",
    "- Marginal histograms of parameters\n",
    "- ACFs\n",
    "- Acceptance Rate vs MSJD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c022ff2-d0b2-4a28-8ff7-6a939dfd685c",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "- Write a function to store the asymptotic_var for the states and for the parameters.\n",
    "    - We can use this to calculate the effective sample size per unit time of the methods that have been run.\n",
    "- Clean up the code base a bit. It is a mess!\n",
    "    - Extend the PMMH class so that it creates a state container with which to store x, and stores it.\n",
    "    - Create an IMMH class to do marginal inference on just the latent states.\n",
    "    - All algorithms that run the MWG step are running extrememly slowly. Try to fix the bug!\n",
    "    - Think about automating Gibbs sampling of the states so that it has separate MWG steps. This may improve algo performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d38708f-7285-4f7f-b9f3-a5786e790571",
   "metadata": {},
   "source": [
    "Coding stuff:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ebd6e-0a97-4ef3-944b-90ff7cf7ca68",
   "metadata": {},
   "source": [
    "- Write a function that takes as input a completed run `MCMC` object and adds an attribute `mcse` and `ess` for the individual chain. We can use this to evaluate the performance of the MCMC algorithms by taking `ess`/`cpu`\n",
    "- Finish writing the function that converts an MCMC output into an inference_data object. This will be useful for analysing chain outputs using the stats and plots in `arviz`. We can combine runs from multiple chains, and construct pooled estimators of `ess` and `cpu_time`\n",
    "- Write a `PIMH` and `iCSMC` class for standard SSMs, then write them for CDSSMs. Test them to make sure that they work.\n",
    "- Extend `CDSSM_SMC` class in `feynman_kac.py` so that after running the filter, it transforms the paths in the particle history object, instead of applying the transform at each time step in the filter.\n",
    "- Start building out the functionality for inference in higher dimensions.\n",
    "- Can run nice experiments with `online_smoothing.py` and `hybrid_paris.py` as benchmarks. \n",
    "- Fix the issues in `parallel_filtering.py` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd9e29-07eb-4a53-ae8f-79d912d129b0",
   "metadata": {},
   "source": [
    "To do:\n",
    "\n",
    "- Store log posterior and acceptance rate information in the 'summary_stats' group\n",
    "- Don't worry about the acceptance rates: they vary at differnt timesteps for NUTS runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b55c10",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# TS_MvOrnsteinUhlenbeck results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471afd24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## General Comments:\n",
    "\n",
    "- The filtering and smoothing is working for both the Bootstrap Reparameterised methods and the Forward Reparameterised methods.\n",
    "- The bootstrap reparameterised methods perform slightly worse than the standard boostrap in the filtering, due to the introduction of bias in the sampling.\n",
    "- The forward reparameterised methods outperform the standard bootstrap and bootstrap reparameterised methods in the filtering. This translates to outperformance in the smoothing.\n",
    "- The 'better' forward proposals outperform more standard/naive choices of forward proposals (e.g the OUP vs NDBBrP)\n",
    "- The backward proposals don't work, likely because there is an issue with the end point proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733669a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Bootstrap Reparameterised Models\n",
    "\n",
    "### Filtering\n",
    "\n",
    "- For all of the Bootstrap Reparameterised filters, the performance of the estimators of the filtering means/marginal likelihoods is reasonable.\n",
    "    - There is a small amount of additional bias from the bbostrap methods, that will come from sampling using a numerical scheme.\n",
    "    - The level of bias is the same for all reparameterised models.\n",
    "    - This implies that the simulation from the multivariate model SDEs is working well.\n",
    "\n",
    "### Smoothing \n",
    "\n",
    "- When using the Delyon-Hu bridge, the smoothing distribution is correctly captured.\n",
    "- When using a van-der-Meulen Schauer bridge, the smoothing distribution is correctly captured (with some bias) with all choices of auxiliary bridge at times $t>2$\n",
    "- At time $t=2$, in one of the dimensions, all of the van-der-Meulen Schauer bridges fail to correctly move the particles to capture the smoothing distribution.\n",
    "    - This is likely due to the difficulty of the problem: even when the ancestors can be resampled, it may be the case that they cannot selsect a single particle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625b3778",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Forward Reparameterised Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ad828",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Filtering\n",
    "\n",
    "- For all of the Bootstrap Reparameterised filters, the performance of the estimators of the filtering means/marginal likelihoods is reasonable.\n",
    "    - There is a small amount of additional bias from the bbostrap methods, that will come from sampling using a numerical scheme.\n",
    "    - The level of bias is the same for all reparameterised models.\n",
    "    - This implies that the simulation from the multivariate model SDEs is working well.\n",
    "\n",
    "### Smoothing \n",
    "\n",
    "- When using the Delyon-Hu bridge, the smoothing distribution is correctly captured.\n",
    "- When using a van-der-Meulen Schauer bridge, the smoothing distribution is correctly captured (with some bias) with all choices of auxiliary bridge at times $t>2$\n",
    "- At time $t=2$, in one of the dimensions, all of the van-der-Meulen Schauer bridges fail to correctly move the particles to capture the smoothing distribution.\n",
    "    - This is likely due to the difficulty of the problem: even when the ancestors can be resampled, it may be the case that they cannot selsect a single particle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cb0676",
   "metadata": {},
   "source": [
    "## Backward Reparameterised Results\n",
    "\n",
    "There is an issue in the filtering: the performance of all backwared methods is much worse than that of the bootstrap. As a result of this,\n",
    "the performance of the smoothing methods is also poor.\n",
    "\n",
    "It is likely that since the performance is okay for all of the forward smoothing methods, that there is an issue with the proposal of the end point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06fbb4",
   "metadata": {},
   "source": [
    "# Additional algorithms\n",
    "\n",
    "## Online smoothing of pathspace additive functionals\n",
    "\n",
    "The evaluation of pathspace additive functionals is useful, as it can be applied to develop:\n",
    "\n",
    "- An Online EM algorithm for maximum likelihood\n",
    "    - This requires evaluation of the smoothing expectation of the joint likelihood in the E-step.\n",
    "    - The M-step is non-trivial on the pathspace.\n",
    "- An Online gradient ascent algorithm for Maximum likelihood\n",
    "    - This requires evaluation of the score function of the model at each update.\n",
    "        - Using Fisher's identity, we can evaluate the score by integrating the gradient of the joint likelihood \n",
    "            w.r.t the smoothing distribution.\n",
    "\n",
    "## Rejection-based SMC algorithms\n",
    "\n",
    "If we are able to come up with an upper bound for $M_t(z_t|z_{t-1})G_t(z_{t-1}, z_t)$, then we can implement the following algorithms:\n",
    "\n",
    "- FFBS-Reject (Offline smoothing) (Need to define method) `fk.upper_bound_trans(t)`\n",
    "- PaRIS algorithm (Online smoothing) (Need to define method) `fk.ssm.upper_bound_log_pt(t)` \n",
    "\n",
    "These methods do smoothing at a cost of $\\mathcal{O}(NT)$ where $N$ is the number of particles. Non-trivial to come up with an upper bound for the forward and backward methods.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
